---
title: "Fast and reliable use of numerical solvers"
date: "`r Sys.Date()`"
author: "Juho Timonen, Ben Bales, Harri Lähdesmäki and Aki Vehtari"
link-citations: true
output:
  html_document:
    number_sections: true
    toc: true
    toc_float: true
    toc_depth: 4
    highlight: pygments
    theme: cosmo
    css: "style.css"
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load, message=FALSE}
# Requirements
require(rstan)
library(tidyverse)
require(bayesplot)
require(ggplot2)
require(loo)
require(stats)
require(posterior)
init_rng_seed <- 999
set.seed(init_rng_seed) # rng seed for data simulation
```

The required `posterior` package is not in CRAN but can be installed via 
`install.packages("posterior", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))`.

# Introduction

Assume we have a Bayesian model that involves an ODE or
PDE (ordinary or partial differential equation) system with uncertain
parameters. When performing Bayesian inference for these parameters 
(and possible other model parameters) using Stan,
the system needs to be solved numerically on each log posterior probability
evaluation. Furthermore, Stan needs to compute gradients for these solutions.
Try as we might, these computations are often expensive and frequently become
the limiting factors in whatever model they are involved in. 

The numerical methods for ODEs and PDEs usually involve some discretization in
space/time, which affects the solver accuracy. Denser discretization means
more accuracy but also more computation. Alternatively, methods can estimate
their error and adapt their step size so that a given tolerance is achieved.
The latter is what the built-in ODE solvers in Stan do.

The simplest things we might do to speed up our calculations are lowering
the timestep, coarsening the discretization, or increasing the tolerance of the
solvers. That immediately leaves us with the question, is this okay? Has
changing the numerical method affected our parameter inference results? Was our
original method giving correct inference results to begin with? Are the default
tolerances in Stan suitable for the problem at hand?

The solution provided by a numerical method is always an approximation to the
true solution, which is general does not have a closed form. This is why our
posterior probability density evaluations are also approximations and
the whole MCMC inference can be thought to be biased to some degree. However,
we can think that the inference results are correct if making the numerical
method more accurate does not affect the statistical properties of the
posterior draws.

How can something like this be checked? The first problem is that it might not
be computationally very attractive to run sampling repeatedly, gradually
increasing the solver accuracy. If the model parameters are fixed,
we can verify that the solution at all points in space/time is
appropriately close to a more accurate reference solution. That isn't so much
of a problem in and of itself, but we're doing statistics, and so we need to
know that the solution is accurate enough across all relevant parts of parameter
space. Additionally, it is not known beforehand where the 
"relevant parts of parameter space" are!

The problem of validating the use of a numerical method for a Bayesian model
is therefore significantly more complicated than in the classical numerical
analysis world. The point of this case study is to show how by adding one
additional tool, namely Pareto-Smoothed Importance Sampling (PSIS) [@yao2018;
@vehtari2019], we can solve this problem.

# Workflow

Let $M$ be the model for which we would like to perform inference, but which we
cannot evaluate since the likelihood is defined implicitly through an
ODE or PDE system that is not analytically tractable. MCMC inference for $M$
can be seen actually as inference for another model $M_{high}$, which
is the same model as $M$ but using a numerical solver, and can therefore be
evaluated.

Our workflow addresses the problem of defining the high-precision numerical
method in $M_{high}$ so that $M_{high}$ can trusted to have essentially the
same posterior as $M$. We define a way to perform inference for $M_{high}$ 
without needing to compute gradients or HMC trajectories for it. This involves
another model $M_{low}$, which is again the same model, except that $M_{low}$
uses a cheaper and less accurate numerical methods (or just looser tolerances
and/or coarser discretization) to compute the required ODE or PDE solutions,
and is therefore faster to fit. The posterior densities are denoted $p_{low}$
and $p_{high}$, respectively.

To understand how PSIS comes into play, we must first discuss importance
sampling. If we want to compute expectations with the high precision model, we
can take draws from the low precision models and reweight these according to the
importance weights $\frac{p_{high}}{p_{low}}$. If these models are too
different, then the reweighting will produce noisy estimates that are not
useful. PSIS and particularly the Pareto $k$-diagnostic (denoted $\hat{k}$), is
the tool that tells us when we can or cannot rely on the importance weights. If
$\hat{k} < 0.5$ we are safe to do the importance sampling, if $\hat{k} < 0.7$
the importance sampling will start to converge more slowly, and if
$\hat{k} > 0.7$ the importance sampling estimates are unreliable. For simplicity
we will only consider the $\hat{k} < 0.5$ threshold.

Ideally, $M_{high}$ would involve a numerical method that we can trust
completely in all parts of the parameter space so that, as long as
$\hat{k} < 0.5$, importance weights can be used to reweight the low precision
approximation $p_{low}$ to the high precision approximation $p_{high}$. We can
think of $M_{high}$ as a reference model, because it is the baseline to which
we compare. It is difficult in practice to know if a given model is a good
reference model in all parts of parameter space, due to curse of dimensionality
and the fact that analysed system can have different properties in different
parts of the parameter space. For example, ODEs can qualitatively change their
behaviour as a function of parameters (bifurcation), or become stiff or
chaotic in some parameter regions. Accuracy can be checked at a given set of
parameters fairly easily, but not over a high dimensional parameter space.
Under these conditions it is necessary to compromise to develop a reference
model that works only over a range of parameter space, but even then it is hard
to know *a priori* what range that is.

We propose the following workflow:

::: {#workflow .box}
1. Generate draws from $p_{low}$.
2. Tune the numerical method in $M_{high}$ so that it is reliable at
these draws. All application specific knowledge and classical numerical
analysis can be used here.
3. Compute importance weights $\frac{p_{high}}{p_{low}}$
4. Compute the $\hat{k}$ diagnostic. If $\hat{k} > 0.5$, raise precision of
the numerical method in $M_{low}$ and go back to step 1.
5. Resample draws of $M_{low}$, using importance weights to get draws from
   $M_{high}$, and therefore essentially draws from $M$.
:::

The next two sections of this case study outline how to apply this workflow to
do fast but reliable inference for

1. an ODE model using a built-in Stan solver
2. a PDE model using a custom solver that does not have explicit tolerance
controls

The importance sampling diagnostics are handled with the
[`loo`](https://mc-stan.org/loo) package and the resampling is handled with the
[`posterior`](https://github.com/stan-dev/posterior) package.

# SIR example (ODE)

Here we study a classic Susceptible-Infected-Recovered (SIR) model of disease
spread. The code is adapted from that in the Stan Case Study [@grinsztajn2020], 
which provides an introduction to disease
transmission modeling in Stan in general.

For the purposes of this case study, the goal is to use a very low
precision ODE solver to do inference and then check it afterwards against
a high precision solver. This is useful in practice if sampling with the
high precision solver itself would take an inordinate amount of time.

The states of the ODE are amounts of susceptible (S), infected (I) and recovered
(R) people. The dynamics are given by the ODE system:

\begin{align}
    \frac{dS}{dt} &= -\beta \cdot I \cdot \frac{S}{N} \\
    \frac{dI}{dt} &=  \beta  \cdot I \cdot \frac{S}{N} - \gamma \cdot I \\
    \frac{dR}{dt} &=  \gamma \cdot I
\end{align}

The parameters $\beta$ and $\gamma$ will be estimated from time series
observations of the number of infected people (I).

## Solving the system

First, we will import a function from `stan/sir.stan` for solving the ODE.
It uses the `integrate_ode_rk45` function that is built into Stan.

```{r sir_model, message=FALSE, results=FALSE}
model <- stan_model("stan/sir.stan")
expose_stan_functions(model)
```

Second, we define various useful constants for our problem (the initial
conditions, etc.) and a wrapper function for the Stan function that solves
the ODE. The actual function exported from Stan is a bit awkward so we rewrap
it here in a way that the only options we will expose are parameters and
tolerance arguments.

```{r sir_solve}
N <- 16 # number of days measured
M <- 1000 # population size
I0 <- 20 # number of infected on day 0
initial_conditions <- c(M - I0, I0, 0) # S, I, R on day 0
ts <- seq_len(N) # measurement times
theta_true <- c(1, 0.2) # true parameter values

# Solve the SIR system and format result array
# - theta = [beta, gamma]
solve_sir <- function(theta, rtol, atol, max_num_steps) {
  stan_solve_sir(
    initial_conditions, ts, theta,
    c(0.0), M, rtol, atol, max_num_steps
  )
}
```

The ODE solvers in Stan are adaptive time step solvers, which estimate the
local error of the solution and adapt their step size so that the local error
estimates are less than `atol + rtol * abs(y)`, where `y` is the ODE solution,
and `atol` and `rtol` are called absolute and relative tolerance,
respectively. These tolerances need to be given, and affect both the accuracy
and computational cost of the solution. In general, `rtol` is the tolerance on
the relative error the solver can make when `y` is far from zero. When `abs(y)`
is small (of the order of `atol` or smaller), there is no need to achieve the
relative tolerance.

A third control parameter, `max_num_steps`, determines the maximum number of
steps that can be taken to achieve the tolerance. In practice, we have
observed that setting this to a much smaller value than the default can
lower the warmup times of some chains by several orders of magnitude. This can
be because it possibly helps in rejecting or quickly getting out of 
initial low-probability parameter regions, where the ODE solutions blow up and achieving the tolerances would require a much larger number of steps than in
the good parameter region.

We can just pick a couple tolerances and quickly run and plot a solution
just to get a feel for what the system looks like:

```{r sir_plot}
plot_sir <- function(y) {
  y %>%
    as_tibble() %>%
    setNames(c("I")) %>%
    mutate(Day = ts) %>%
    ggplot(aes(Day, I)) +
    geom_line() +
    geom_point() +
    ylab("Infected people")
}

plot_sir(solve_sir(theta_true, 1e-4, 1e-4, 100))
```

In this model we are making noisy measurements of the number of infected
people (I) at each day.

## Generating test data

If we're going to generate data from our model we better have an accurate ODE
solver, otherwise we're just generating data from some weird approximate model.

The simplest way to check that an `atol` and `rtol` are suitable is to do
a solve at one tolerance level, repeat the solve at a much smaller (more
precise) tolerance, and then look at the maximum absolute error at any output
point. We will create a function to do this automatically:

```{r sir_check}
check_reliability <- function(theta, rtol, atol, max_num_steps) {
  y_hat <- solve_sir(theta, rtol, atol, max_num_steps)
  y_hat2 <- solve_sir(theta, rtol / 10, atol / 10, max_num_steps)
  max_abs_err <- max(abs(y_hat - y_hat2))
  return(max_abs_err)
}
```

We can study the maximum absolute error compared to a solution with 10 smaller
tolerances, as a function `tol = atol = rtol`.

```{r sir_tols, fig.width=7, fig.height=3.5}
errors <- c()
tols <- 10^(-c(1:12))
for (tol in tols) {
  errors <- c(errors, check_reliability(theta_true, tol, tol, 1e8))
}

qplot(tols, errors, geom = c("point", "line")) +
  scale_x_log10() +
  scale_y_log10()
```

From this and our prior knowledge of infectious diseases, we assert
that $10^{-6}$ is a good enough value to use for `atol` and `rtol` during
simulation. Certainly we do not expect have a count of the infected
population accurate to $10^{-4}$ people.

We assume a negative binomial measurement model to get an observed number
of infected people (cases) at each time point.

```{r sir_noise, fig.width=7, fig.height=4.8}
atol <- 1e-6
rtol <- 1e-6

dispersion <- 5 # noise parameter for negative binomial
mu <- solve_sir(theta_true, atol, rtol, 1e8)
y <- stats::rnbinom(length(ts), mu = mu, size = dispersion)

tibble(t = ts, mu = mu, y = y) %>%
  ggplot() +
  geom_line(aes(t, mu), col = "firebrick") +
  geom_point(aes(t, y)) +
  xlab("Day") +
  ylab("Infected people") +
  ggtitle("Simulated data as points \nUnderlying solution as lines")
```

## Applying the workflow

As a reminder, our mission in fitting this ODE is to use a low precision
solver. It is always tempting to use low precision solvers when working
with ODEs because they run faster. The difficulty becomes how to deal with
the coarser approximation. Does the lower precision cause an effect that
matters? If so, can it be corrected and how? These are the questions the
workflow here will allow us to answer.

### Generating draws from $p_{low}$

The first step in the workflow is to take any low precision approximation 
($M_{low}$) and fit the data. Remember, all our numerical methods are
approximations, and so we refer to this model specifically as a low precision
model. We will check it against a higher precision model later. In this case,
we will use `rtol = 1e-4`, `atol = 1e-3`.

```{r sir_fit, fig.width=7, fig.height=4.5, results=FALSE}
rtol_low <- 1e-4
atol_low <- 1e-3
max_num_steps_low <- 100

fit <- rstan::sampling(model, list(
  N = length(ts),
  M = M,
  ts = ts,
  y = y,
  initial_conditions = initial_conditions,
  rtol = rtol_low,
  atol = atol_low,
  max_num_steps = max_num_steps_low
))
```


```{r sir_summary}
print(fit, pars = c("beta", "gamma", "phi"))
```

### Developing a reference model

Before we can check if the importance sampling correction is possible, we
need to have a reference model ($M_{high}$) to compare against. That means we
need a version of the model with tolerances such that it is suitably accurate
across all the posterior draws generated from the low precision model.

In this case, `rtol = atol = 1e-6` was accurate enough generating the data,
so let's check if it is accurate in all the draws in this posterior.

```{r sir_tune}
atol_high <- 1e-6
rtol_high <- 1e-6
max_num_steps_high <- 1e6
draws <- rstan::extract(fit, pars = c("beta", "gamma", "phi"))
phi_draws <- draws$phi
theta_draws <- cbind(draws$beta, draws$gamma)
num_draws <- length(phi_draws)

# Compute differences
errors <- c()
for (i in 1:num_draws) {
  mae <- check_reliability(
    theta_draws[i, ], atol_high, rtol_high,
    max_num_steps_high
  )
  errors <- c(errors, mae)
}
```

We can plot this as a distribution and see that `rtol = atol = 1e-6` keeps
us under an absolute error of one milliperson. This seems accurate enough.

```{r sir_hist, message=FALSE}
qplot(errors, geom = "histogram")
```

### Computing importance weights

With the reference model in place, it is time to compute the importance
weights $\frac{p_{high}}{p_{low}}$ for each post-warmup draw. This is simple:
just compute the log density of the reference model and the log
density of the low precision model and take the difference (we work
with the log of the importance ratios $\log p_{high} - \log p_{low}$
for numeric stability).

The hidden downside is that it might take some time to compute the log
densities of the reference model for each draw. It should still be way faster
than sampling with the reference model itself, since we don't need to compute
gradients, HMC trajectories, evaluate proposals that can be rejected, and 
skip the whole warmup period. Therefore we likely won't have to try to do
accurate ODE solves in the ill-behaved parameter regions, where a huge number
of steps would be needed to achieve the ODE tolerances. Another upside is that
the calculations can be done in parallel for each draw.

If the priors are kept the same between the reference and low precision
model, then those can be left out of this calculation (they will cancel).

```{r sir_weights}
log_lh_low <- rep(0, num_draws)
log_lh_high <- rep(0, num_draws)
for (i in seq_len(num_draws)) {
  y_hat_low <- solve_sir(
    theta_draws[i, ], rtol_low, atol_low,
    max_num_steps_low
  )
  y_hat_high <- solve_sir(
    theta_draws[i, ], rtol_high, atol_high,
    max_num_steps_high
  )
  log_lh_low[i] <- sum(dnbinom(y,
    size = phi_draws[i],
    mu = y_hat_low, log = TRUE
  ))
  log_lh_high[i] <- sum(dnbinom(y,
    size = phi_draws[i],
    mu = y_hat_high, log = TRUE
  ))
}
log_weights <- log_lh_high - log_lh_low
```

We can plot the log importance ratios and see they are all close to zero
(which means out approximation was not too bad).

```{r sir_weight_hist, message=FALSE}
qplot(log_weights, geom = "histogram")
```

### Computing $\hat{k}$ diagnostic

With the importance ratios calculated, we can check if they are usable or not
with the PSIS $\hat{k}$ diagnostic.

```{r sir_pareto}
psis1 <- loo::psis(log_weights)
print(psis1)
```

$\hat{k} < 0.5$, and so importance sampling should be reliable.

### Resampling

At this point we have a weighted set of posterior draws. It
is usually easier to work with a set of draws than a set of weighted draws, so
we can resample our weighted draws to become a set of unweighted draws using
`posterior::resample_draws`. The effective sample size will be slightly
lowered by such a resampling, but unweighted draws are really more convenient
to work with.

Just because it is possible to do an importance sampling correction on a set
of draws does not mean that unweighted statistics on these draws are safe
to use. In this case, the results are not much different, but it should not
be forgotten:

```{r sir_resample}
w <- exp(log_weights)
draws_list <- rstan::extract(fit, c("beta", "gamma", "phi"))
draws_df <- posterior::as_draws_df(draws_list)
resampled_df <- posterior::resample_draws(draws_df,
  weights = w
)

print(draws_df %>% posterior::summarize_draws())
print(resampled_df %>% posterior::summarize_draws())
```

<!-- WEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE -->

# Heat Equation Example (PDE)

In this example we consider the diffusion of heat ($u(t, x)$) in a rod
($x \in [0, L]$).

For the purposes of this case study, the goal is to use a PDE solver in a
model that has no automatic error control, only fixed discretization controls.
This problem comes up from time to time when problems demand a custom solver
be written -- it is not always easy to tack on error control algorithms.

In this hypothetical experiment, the rod is cooled to room
temperature and then heated from the left side. After some time the temperature
profile of the rod is measured and from this the thermal diffusivity $K$ will 
be estimated.

The dynamics are governed by the 1D heat equation:

\begin{align}
\frac{\partial u}{\partial t} &= K \cdot \frac{\partial^2 u}{\partial x^2} \\
u(0, x) &= 0 \\
u(t, 0) &= 1 \\
u(t, L) &= 0
\end{align}

## Solving the system
All of the computations in this example are going to be done with a method of
lines discretization of this problem and a backwards Euler integrator. The
appropriate math is described in the online lecture notes
[ATM 623: Climate Modeling](http://www.atmos.albany.edu/facstaff/brose/classes/ATM623_Spring2015/Notes/Lectures/Lecture16%20--%20Numerical%20methods%20for%20diffusion%20models.html) by Brian E. J. Rose, though any introductory PDE reference should suffice.

For convenience we have defined a Stan function that solves equations above
and computes the measured temperatures in the system given a timestep, a
spatial discretization, a hypothetical diffusivity, a measurement time, and a
list of measurement points.

```{r diffusion_model, message=FALSE, results=FALSE}
model <- stan_model("stan/diffusion.stan")
expose_stan_functions(model)
```

```{r diffusion_solve}
dt <- 1.0
Nx <- 10
K <- 1e-1
T_meas <- 0.1
x_meas <- c(-1.0, 0.01, 0.5, 0.99, 1.0, 2.0)

solve_pde(dt, Nx, K, T_meas, x_meas)
```

The function has the signature:

```
vector solve_pde(dt, Nx, K, T_meas, x_meas)
```

with arguments:

* `dt` - Timestep
* `Nx` - Number of interior points in spatial discretization
* `K` - Thermal diffusivity
* `T_meas` - Measurement time
* `x_meas` - Measurement points

Assume a true thermal diffusivity $K_{true} = 0.05$ and that we measure the
temperature in the rod at `Nx` points evenly spaced on the rod. We will
generate data under these conditions and try to recover the diffusivity later.

First, let's set up constants and plot a possible solution with measurement
points:

```{r diffusion_setup, fig.width = 6.75, fig.height = 5}
dt <- 1e-1
Nx <- 5
L <- 1.0

x_meas <- seq(0.0, L, length = 7)[2:6]
T_meas <- 1.0
K_true <- 0.015

# For these calculations pretend we are measuring everywhere so we can
#  see the whole solution
x <- seq(-0.1, 1.1, length = 100)
u0 <- c(rep(1.0, sum(x <= 0.0)), rep(0.0, sum(x > 0.0)))
uT <- solve_pde(dt, Nx, K_true, T_meas, x)

# Solve at only the measurement points
mu <- solve_pde(dt, Nx, K_true, T_meas, x_meas)

# Plot
tibble(x = x, `u(t = 0)` = u0, `u(t = T)` = uT) %>%
  gather(Legend, u, -x) %>%
  ggplot(aes(x, u)) +
  geom_line(aes(color = Legend, group = Legend)) +
  geom_point(data = tibble(x = x_meas, u = mu)) +
  ggtitle("Measurement points indicated in black\nDashed lines indicate boundary of sample") +
  geom_vline(aes(xintercept = 0.0), linetype = "dashed") +
  geom_vline(aes(xintercept = L), linetype = "dashed")
```

The red line shows the initial conditions. Because the solution is actually
discretized, to only five points on the rod (seven including the boundaries)
we do linear interpolation to get the values in the intermediate points (which
makes the boundary look a bit strange).

The teal points show the distribution of heat in the rod at time `t = T`, where
we plan to take measurements (indicated by the black dots) and make an
inference about the unknown thermal diffusivity of the rod.

## Generating test data

Now that we can compute solutions to this problem, our first question will
be is a given solution accurate enough? The simple way to check this is by
computing the solution again at a higher space/time resolution and checking
the difference.

We can define a convenience function that for a given discretization and
experimental configuration computes a solution and also another solution at
higher precision and returns the maximum absolute error.

```{r diffusion_check}
# Function to help determine if dt and Nx are small enough at given K
check_reliability <- function(dt, Nx, K, T_meas, x_meas) {
  mu <- solve_pde(dt, Nx, K, T_meas, x_meas)
  mu_more_accurate <- solve_pde(dt / 2.0, 2 * Nx, K, T_meas, x_meas)
  max_abs_err <- max(abs(mu_more_accurate - mu))
  return(max_abs_err)
}

# Check at K = K_true
check_reliability(dt, Nx, K_true, T_meas, x_meas)
```

Is that error good or is that bad? That is something that will need to be
determined in the context of the application. In this case we are going assume
a measurement noise of $0.1$, and so we should get our numerical error quite a
bit below that.

```{r diffusion_check2}
dt <- 0.01
Nx <- 40
check_reliability(dt, Nx, K_true, T_meas, x_meas)
```

This seems good enough for now, but you might further refine your solution. Now
to simulate data:

```{r diffusion_noise}
sigma <- 0.1
noise <- rnorm(length(x_meas), 0, sigma)
y <- solve_pde(dt, Nx, K_true, T_meas, x_meas) + noise
```

## Applying the workflow

### Generating draws from $p_{low}$

Now that we have simulated data, it is time to do inference. The first step,
similarly as for the ODE, is to fit an initial approximate model to our
data. Again, all our calculations are approximations, and so we refer to this
model as a low precision model because we will check it against a higher
precision model later.

Assume we are very impatient and want this computation to finish quickly so
that we use only one timestep and one spatial point of resolution in our
discretization:

```{r diffusion_fit1, results=FALSE}
dt_low <- 1.0
Nx_low <- 1
fit <- rstan::sampling(model,
  data = list(
    dt = dt_low,
    Nx = Nx_low,
    N_meas = length(x_meas),
    T_meas = T_meas,
    x_meas = x_meas,
    y = y
  ),
  control = list(adapt_delta = 0.95),
  cores = 1
)
```

Let us look at our results:

```{r diffusion_summary1}
print(fit, pars = c("K", "sigma"))
```

We remember from earlier that $K_{true} = 0.1$, $\sigma_{true} = 0.1$,
so something is off. We will diagnose this using our approximation tools.

### Developing a reference model

Again, to check if the importance sampling correction can be done, we need
a reference model that works for all the posterior draws we got from the
low precision model. We can develop the reference model using the same
technique we did previously (guess a high precision, and check the maximum
absolute error between that high precision model and one of even higher
precision).

```{r diffusion_tune1}
dt_high <- 0.01
Nx_high <- 100

K_draws <- rstan::extract(fit, "K")$K
sigma_draws <- rstan::extract(fit, "sigma")$sigma
num_draws <- length(K_draws)

# Compute differences
errors <- c()
for (i in 1:num_draws) {
  mae <- check_reliability(dt_high, Nx_high, K_draws[i], T_meas, x_meas)
  errors <- c(errors, mae)
}
```

With a simple one parameter model we can plot our approximate errors as a
function of K (so we know the solution is suitable everywhere).

```{r diffusion_errors1}
error_plot <- function(K, mae) {
  ylab <-
    df <- data.frame(K, mae)
  ggplot(df, aes_string(x = "K", y = "mae")) +
    geom_point(col = "#1864de", alpha = 0.5) +
    xlab("K") +
    ylab("Max. absolute error")
}
error_plot(K_draws, errors)
```

The errors here seem low enough.

### Computing importance weights

The importance weights $\frac{p_{high}}{p_{low}}$ are computed on the log
scale. The priors cancel out so we only need to work with log likelihoods.

Again, this step looks simple in this example, but in practice it might be
more complicated. It is possible that the reference calculation is done
with an entirely different piece of software. For instance, with a PDE
perhaps a the reference solution is computed with a well-tested FEM solver
in a different software environment entirely.

```{r diffusion_weights1}
# Define a function
compute_log_weights <- function(
                                dt_low, Nx_low, dt_high, Nx_high,
                                K_draws, T_meas, x_meas, y_meas) {
  log_lh_low <- rep(0, num_draws)
  log_lh_high <- rep(0, num_draws)
  for (i in seq_len(num_draws)) {
    mu_low <- solve_pde(dt_low, Nx_low, K_draws[i], T_meas, x_meas)
    mu_high <- solve_pde(dt_high, Nx_high, K_draws[i], T_meas, x_meas)
    log_lh_low[i] <- sum(dnorm(y_meas, mu_low, sigma_draws[i], log = TRUE))
    log_lh_high[i] <- sum(dnorm(y_meas, mu_high, sigma_draws[i], log = TRUE))
  }
  log_weights <- log_lh_high - log_lh_low
  return(log_weights)
}

# Apply function
log_weights <- compute_log_weights(
  dt_low, Nx_low, dt_high, Nx_high,
  K_draws, T_meas, x_meas, y
)
```

### Computing $\hat{k}$ diagnostic

If the $\hat{k}$ diagnostic is not low enough, it is not possible to do the
importance sampling correction and we need to recompute our posterior with a
higher resolution model. The `loo` package computes the $\hat{k}$ diagnostic
for us:

```{r diffusion_pareto1, fig.width=6, fig.height=4.5}
psis2 <- loo::psis(log_weights)
print(psis2)
```

Oh no! $\hat{k} > 0.5$, and it turns out modeling this process with one timestep
and one spatial point was not a good idea. This means we need to up the precision
in the low resolution model and go back to *Step 1*.

### Repeating the loop

```{r diffusion_fit2, results=FALSE}
dt_low <- 0.1
Nx_low <- 10
fit <- rstan::sampling(model,
  data = list(
    dt = dt_low,
    Nx = Nx_low,
    N_meas = length(x_meas),
    T_meas = T_meas,
    x_meas = x_meas,
    y = y
  ),
  control = list(adapt_delta = 0.95),
  cores = 1
)
```

Again, we can check our regular diagnostics:

```{r diffusion_summary2}
print(fit, pars = c("K", "sigma"))
```

Again, we verify our reference solution:

```{r diffusion_tune2}
K_draws <- rstan::extract(fit, "K")$K
sigma_draws <- rstan::extract(fit, "sigma")$sigma
num_draws <- length(K_draws)

# Compute differences
errors <- c()
for (i in 1:num_draws) {
  mae <- check_reliability(dt_high, Nx_high, K_draws[i], T_meas, x_meas)
  errors <- c(errors, mae)
}

# Plot
error_plot(K_draws, errors)
```

And again we can compute the importance ratios and run the PSIS diagnostics on
them:

```{r diffusion_weights2}
log_weights <- compute_log_weights(
  dt_low, Nx_low, dt_high, Nx_high,
  K_draws, T_meas, x_meas, y
)
psis3 <- loo::psis(log_weights)
print(psis3)
```

And this time $\hat{k} < 0.5$, so we are good enough!

### Resampling draws

At this point we have a weighted set of posterior draws. Again it
is usually easier to work with a set of draws than a set of weighted draws, so
we resample our weighted draws using `posterior::resample_draws`.

```{r diffusion_resampling}
w <- exp(log_weights)
draws_list <- rstan::extract(fit, c("K", "sigma"))
draws_df <- posterior::as_draws_df(draws_list)
resampled_df <- posterior::resample_draws(draws_df,
  weights = w
)

print(draws_df %>% posterior::summarize_draws())
print(resampled_df %>% posterior::summarize_draws())
```

<!-- WEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE -->


# Conclusions

And that is that! Happy approximating!

# Computation environment

```{r computational_environment, echo=FALSE}
sessionInfo()
cat(paste0("Initial rng seed was ", init_rng_seed, "\n"))

# Result check
pareto_k_check <- function(res, expected_ok = TRUE) {
  arg_name <- deparse(substitute(res))
  khat <- res$diagnostics$pareto_k
  msg <- paste0("In ", arg_name, ", pareto_k was ", round(khat, 5), "\n")
  cat(msg)
  return(khat)
} 
khat1 <- pareto_k_check(psis1)
khat2 <- pareto_k_check(psis2)
khat3 <- pareto_k_check(psis3)
if (khat1 > 0.5 || khat2 < 0.5 || khat3 > 0.5) {
  stop("Results inconsistent with text!")
}
```

# References
