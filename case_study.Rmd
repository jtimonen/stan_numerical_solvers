---
title: "Fixing Approximations"
date: "`r Sys.Date()`"
author: "Ben Bales and Juho Timonen" 
output: 
  html_document:
    toc: true
    theme: yeti
    highlight: textmate
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load, include=FALSE}
require(rstan)
```

## 1. Introduction

Assume we have a Bayesian model:

\begin{equation}
p(\theta \mid x) \propto p(x \mid \theta) p(\theta)
\end{equation}

Try as we might to make this model go fast, sometimes it just doesn't. In fact,
often it doesn't. There are numerous reasons this might happen:

  1. Difficult to sample posterior geometry
  2. Bug in the code
  3. Model does not fit the data well
  4. Lots of data
  5. etc...

The particular situation we want to discuss here is one where you are happy
with your model but there are some difficult calculations you want to
approximate for performance reasons.

This comes up relatively often when dealing with integrals and differential
equations in Stan. Stan comes with a couple common ODE integrators and one
quadrature scheme in the language itself, but if those prepackaged solutions
do not work, something else must be done. Usually what happens is an approximate
quadrature or integrator gets used that is much faster or more stable than the
reference implementation in Stan. The question immediately becomes, how do
I know my approximation is good enough?

The goal of this case study is to figure out if your approximation is good
enough, and, if possible use importance sampling to correct for any bias in
the results. This is using techniques from the papers
[@yao2018], [@vehtari2019], and
the `loo` software package (https://mc-stan.org/loo).

## 2. 1D Diffusion Example

### 2.1  Problem

First, let's start with a problem where Stan is slow. Let's pretend that we are modeling heat diffusion on a rod $x \in [0, L]$ as a partial differential equation
\begin{equation}
\frac{\partial u}{\partial t} = \kappa \cdot \frac{\partial^2 u}{\partial x^2}
\end{equation}
with boundary conditions
$$
u(t, x = 0) = 0 \\
u(t, x = L) = 1
$$
and initial heat distribution
\begin{equation}
u(t = 0, x) = 
\begin{cases}
0 & \text{ if } \hspace{0.5cm} 0 \leq x \leq \frac{L}{2} \\
1 & \text{ if } \hspace{0.5cm} \frac{L}{2} < x \leq 1
\end{cases}
\end{equation}
The thermal diffusivity parameter $\kappa$ is unknown. The goal is to eastimate it from noisy measurements of the concentration $u(t,x)$ at time $t = T$.

### 2.2 Simulating data
We generate test data using $\kappa_{true} = 0.25$. We can easily discretize the system using [method of lines](https://en.wikipedia.org/wiki/Method_of_lines) and generate some sample data in R:

```{r sim_functions}
# Function to solve u(t,x)
solve_u <- function(u_init, T_end, dt, dx) {
  u <- u_init
  for (t in seq(0, T_end, by = dt)) {
    u_new <- u
    for (i in 2:(length(u) - 1)) {
      u_new[i] <- dt * (u[i + 1] - 2 * u[i] + u[i - 1]) / (2 * dx) + u[i]
    }
    u <- u_new
  }
  return(u)
}

# Function to plot the solution
plot_u <- function(x, u_init, u_end, col1 = "gray30", col2 = "#198bff") {
  lwd <- 2
  plot(x, u_init, type = 'l', col = col1, main = 'Heat distribution on [0,L]', ylab = 'u(t,x)', xaxt = "n", lwd = lwd)
  lines(x, u_end, col = col2, lwd = lwd)
  legend(0.8, 0.4, legend = c("t = 0", "t = T"), col = c(col1, col2), lty = c(1,1), lwd = c(lwd, lwd))
  axis(1, at = c(0, 0.5, 1.0), labels = c("0", "L/2", "L"))
}
```


```{r simulate, fig.width = 6.5, fig.height = 4.5}
# Setup
L <- 1        # rod length
T_end <- 1    # time interval length
Kappa <- 0.25 # true value of kappa
dx <- 1e-2    # discretization step in x
dt <- 1e-4    # discretization step in t

# Define initial heat distribution u(t=0, x)
x_grid <- seq(0, 1, by = dx)
u_init <- x_grid > (0.5 * L)

# Solve u(t=T, x) and plot
u_end <- solve_u(u_init, T_end, dt, dx)
plot_u(x_grid, u_init, u_end)
```

Now, if you're familiar with diffusion equations you might say, 'Oh, you
fools, you've done a forward Euler method on a diffusion problem and you're
not even trying to control your error due to your timestep or your spatial
discretization.'

Since you are this far in the case study, we will admit, yes, this is a bad way
to solve the problem. We did it in a pinch because it was easier to do it
this way than worry about an adaptive timestep/adaptive meshsize method.
After all, we are solving a 1D diffusion. We can just make a finer
discretization or use smaller timesteps and see how the error between successive
approximations gets smaller.

But how should we check this in a model in Stan? The error in the solution from
using an approximation is important, but really the error in the posterior is
what matters. That leads to our first question

### 2.3 Is the Approximation Good Enough?

Assume we have a distribution $p_{true}(\theta)$ and $p_{approx}(\theta)$.

$p_{approx}$ is meant to approximate $p_{true}$. If they have the same support
then mathematically we can write expectations over $p_{true}$ as weighted
expectations over $p_{approx}$.

$$
E[f(\theta)] = \int f(\theta) p_{true}(\theta)\\
 = \int f(\theta) \frac{p_{true}(\theta)}{p_{approx}(\theta)}p_{approx}(\theta)d\theta
$$

If we're computing estimates with Monte Carlo, this means we replace
expectations over samples from our true distribution with weighted samples from
the approximate distribution.

$$
E[f(\theta)] \approx \frac{1}{N} \sum_n^N f(\theta_n) \\
\approx \frac{1}{N} \sum_n^N f(\theta_n) \frac{p_{true}(\theta)}{p_{approx}(\theta)}
$$
This is useful in MCMC when it is easier to sample from the approximation
than the truth. The ratios $\frac{p_{true}}{p_{approx}}$ are called importance
ratios. Perhaps unsurprisingly, this correction does not always work with
Monte Carlo. Indeed, if the difference in $p_{true}$ and $p_{approx}$ is too
great, then the importance ratios will be mostly things going to infinity or
zero.

In general, we can diagnose if the expectations will work by examing the
distribution of the importance weights by using the Pareto Smooth Importance
Sampling diagnostic. But first, let's write a Stan model

### 2.4 1D Diffusion in Stan

We define stan model.
```{r model}
model <- stan_model("diffusion.stan")
stan_data <- list(N = length(x_grid),
                  x = x_grid,
                  y = rnorm(u_end, 0.1),
                  L = L,
                  Tf = 0.1,
                  dt = 1e-3,
                  sigma = 0.1)
```
Hello, yes now we fit.

```{r fit, cache=TRUE}
fit <- sampling(model,
                data = stan_data,
                iter = 1000,
                chains = 4,
                cores = 4)
print(fit)
```

### 2.5 Correcting the Approximation
Heck.

## Computation environment

```{r session}
sessionInfo()
```

## References
