---
title: "Fast and reliable use of numerical solvers"
date: "`r Sys.Date()`"
author: "Juho Timonen and Ben Bales"
link-citations: true
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 5
    theme: yeti
    highlight: textmate
bibliography: references.bib
biblio-style: imsmart-nameyear
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load, message=FALSE}
# Requirements
require(rstan)
library(tidyverse)
require(bayesplot)
require(grDevices)
require(ggplot2)
require(loo)
require(stats)
require(posterior)

# The 'posterior' package is required for some computations. It is not in CRAN but can be installed via
# 
#  install.packages("posterior", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
```

# Introduction

Assume we have a Bayesian model that involves a numeric solution to some
ODE or PDE (ordinary or partial differential equation). Try as we might, these calculations are often expensive and frequently becomes the limiting factors in whatever model they are involved in.

The simplest things we might do to speed up our calculations are lower the
timestep, coarsen the discretization, or increase the tolerance of the solvers.

That immediately leaves us with the question, is this okay? Has lowering the
tolerance affected the statistics of our solution? The technical answer is yes,
the practical answer is maybe, and the ideal case is no, but without checking
there's no way of knowing.

And so how can something like this be checked? The numerical approximation
answer is we should compare to a reference model and verify that the solution
at all points in space/time in the approximation are appropriately close
to the reference. That isn't so much of a problem in and of itself, but we're
doing statistics, and so we need to know that the solution is accurate enough
across all relevant parts of parameter space. Additionally, the "relevant parts
of parameter space" is the posterior itself which is not known beforehand!

And so the problem of validating an approximation in a Bayesian model is
significantly more complicated than in the classical numerical analysis world.
The point of this case study is to show how by adding one additional tool 
Pareto-Smoothed Importance Sampling (PSIS [@yao2018], [@vehtari2019]), we can
solve this problem.

# PSIS and computing a reference model

To understand the usefulness of PSIS, we must first discuss importance sampling.
Assume we have two models, $p_{high}$ and $p_{low}$. $p_{high}$ will be an
expensive, high precision model and $p_{low}$ is a cheap, low precision model.
If we want to compute expectations with the high precision model, we can take
draws from the low precision models and reweight these according to the
importance weights $\frac{p_{high}}{p_{low}}$. If these models are too
different, then the reweighting will produce noisy estimates that are not
useful. PSIS and particularly the $\hat{k}$ diagnostic (pronounced k-hat) is
the tool that tells us when we can or cannot rely on the importance weights. If
$\hat{k} < 0.5$ we are safe to do the importance sampling, if $\hat{k} < 0.7$
the importance sampling will start to converge more slowly, and if
$\hat{k} > 0.7$ the importance sampling estimates are unreliable. For simplicity
we will only considier the $\hat{k} < 0.5$ threshold.

Because of this, if $p_{high}$ is a reference model that we trust completely,
our workflow can be:

1. Generate draws from $p_{low}$
2. Compute importance weights $\frac{p_{high}}{p_{low}}$ and $\hat{k}$
diagnostic.
3. Break if $\hat{k} < 0.5$, otherwise raise precision of $p_{low}$
and loop
4. Resample draws using importance weights

As described above though, we never really have a reference $p_{high}$ that
we trust completely because we never really know where exactly to evaluate the
solution. PSIS gives us an answer to this though. Expanding the workflow above
with a new step we get:

1. Generate draws from $p_{low}$
2. Compute a reference solution, $p_{high}$ at these draws
3. Compute importance weights $\frac{p_{high}}{p_{low}}$ and $\hat{k}$
diagnostic.
4. Break if $\hat{k} < 0.5$, otherwise raise precision of $p_{low}$
and loop
5. Resample draws using importance weights

Step #2 should be done using whatever application and method specific numerical
techniques are applicable. In this workflow we have both solved the problem
of picking an appropriate approximation and also the problem of developing a
good reference model.

The rest of this case study is demonstrating how to do this on a simple PDE
model using Stan in combination with the [`loo`](https://mc-stan.org/loo) and
[`posterior`](https://github.com/stan-dev/posterior) software packages.

# Modeling the Heat Equation

In this example we consider the diffusion of heat ($u(t, x)$) in a rod
($x \in [0, L]$). In this hypothetical experiment, the rod is cooled to room
temperature and then heated from the left side. After some time the temperature
profile of the rod is measured and from this the thermal diffusivity is to be
estimated.

The dynamics are governed by the 1D heat equation:

\begin{align}
\frac{\partial u}{\partial t} &= K \cdot \frac{\partial^2 u}{\partial x^2} \\
u(0, x) &= 0 \\
u(t, 0) &= 1 \\
u(t, L) &= 0
\end{align}

All of the computations in this case study are going to be done with a method of
lines discretization of this problem and a backwards Euler integrator. The
appropriate math is described in the online lecture notes
[ATM 623: Climate Modeling](http://www.atmos.albany.edu/facstaff/brose/classes/ATM623_Spring2015/Notes/Lectures/Lecture16%20--%20Numerical%20methods%20for%20diffusion%20models.html) by Brian E. J. Rose, though any introductory PDE reference should suffice.

For convenience we have defined a Stan function that solve our problem
given a timestep, a spatial discretization, a hypothetical diffusivity, a
measurement time, and a list of measurement points and returns the predicted
temperature at each of those measurement points.

```{r}
model <- stan_model("diffusion/diffusion.stan")
expose_stan_functions(model)
```

```{r}
dt = 1.0
Nx = 10
K = 1e-1
T_meas = 0.1
x_meas = c(-1.0, 0.01, 0.5, 0.99, 1.0, 2.0)

solve_pde(dt, Nx, K, T_meas, x_meas)
```

The function has the signature:

```
vector solve_pde(dt, Nx, K, T_meas, x_meas)
```

with arguments:

* `dt` - Timestep
* `Nx` - Number of interior points in spatial discretization
* `K` - Thermal diffusivity
* `T_meas` - Measurement time
* `x_meas` - Measurement points

Assume a true thermal diffusivity $K_{true} = 0.05$ and that we measure the
temperature in the rod at `Nx` points evenly spaced on the rad. We will
generate data under these conditions and try to recover the diffusivity later.

First, let's set up constants and plot a possible solution:

```{r diffusion, fig.width = 6.75, fig.height = 5}
dt = 1e-1
Nx = 5
L = 1.0
# Pretend we're measuring everywhere so we see the full solution
x <- seq(-0.1, 1.1, length = 100)
x_meas <- seq(0.0, L, length = 7)[2:6]
T_meas <- 1.0
K_true <- 0.025

u0 = solve_pde(dt, Nx, K_true, 0.0, x)
uT = solve_pde(dt, Nx, K_true, T_meas, x)

mu_low_res = solve_pde(dt, Nx, K_true, T_meas, x_meas)

plot(x, u0, type = 'l')
lines(x, uT, col = 'red')
points(x_meas, mu_low_res, col = 'blue')
```

The blue points in the solution are the ones we compare to experiment (these
are the parts of the solution we care most about). Our first question will be:
is this solution accurate enough at the blue points? The simple way to check
this is by computing the solution at the blue points at a higher higher
space/time resolution and checking the difference.

Let's double the space and time resolution:

```{r}
mu_high_res = solve_pde(dt / 2.0, 2 * Nx, K_true, T_meas, x_meas)
print(max(abs(mu_high_res - mu_low_res)))
```

Is that good or is that bad? That is something that will need determined in the
context of the application. In this case I am going to generate data with a
standard deviation of $0.1$, and so I want to get my numerical error quite a
bit below that before I am comfortable.

```{r}
dt = 0.01
Nx = 40
mu_low_res = solve_pde(dt, Nx, K_true, T_meas, x_meas)
mu_high_res = solve_pde(dt / 2.0, Nx * 2, K_true, T_meas, x_meas)
print(max(abs(mu_high_res - mu_low_res)))
```

This seems good enough for now, but you might further refine your solution. Now
to simulate data:

```{r}
sigma = 0.1
y = solve_pde(dt, Nx, K_true, T_meas, x_meas) + rnorm(length(x_meas), 0, sigma)
```

## Outline

As we take off our data-generating hats and put on our model-fitting hats, let
us remind ourselves of our original workflow:

1. Generate draws from $p_{low}$

We fit an initial approximate model to our data. All our calculations are
approximations, and so we refer to this model as a low resolution model, because
we will check the calculations we did with it in the following steps.

2. Compute a reference solution, $p_{high}$ at these draws

Once we have posterior draws, we need to compute a high resolution reference
model at each of these points. We will do this by comparing computations with
the low resolution to the high resolution ones at every posterior draw and
increasing the resolution of the reference model until we are happy it is
accurately representing the dynamics.

3. Compute importance weights $\frac{p_{high}}{p_{low}}$ and $\hat{k}$
diagnostic.

At this stage, we can compute importance weights and the $\hat{k}$ diagnostic

4. Break if $\hat{k} < 0.5$, otherwise raise precision of $p_{low}$ and loop

If the $\hat{k}$ diagnostic is not low enough, it is not possible to do the
importance sampling correction and we need to recompute our posterior with a
higher resolution model.

5. Resample draws using importance weights

This is optional. At this point we have a weighted set of posterior draws. It
is usually easier to work with a set of draws than a set of weighted draws, so
we can resample our weighted draws using `posterior::resample_draws`.

## 1. Generate draws from $p_{low}$

Pretend this is measured data and we do not know the true thermal diffusivity.
Perhaps we are greedy and want this computation to finish quickly and so we
fit this with a very aggressive approximation (one timestep, one spatial point):

```{r}
dt = 1.0
Nx = 1

fit <- sampling(model,
            data = list(dt = dt,
                        Nx = Nx,
                        N_meas = length(x_meas),
                        T_meas = T_meas,
                        x_meas = x_meas,
                        y = y),
            cores = 4)
```

Looking at the posterior it appears everything was successful and the chains
mixed. However, we remember from earlier that $K_{true} = 0.1$, so something is
off. We will diagnose this using our approximation tools though.

```{r}
print(fit, pars = c("K", "sigma"))
```

## 2. Compute a reference solution, $p_{high}$ at these draws

Given our posterior draws, we need to a high resolution reference solution
$p_{high}$ that works at every point. This does not mean we need $p_{low}$ to
be accurate. It could be, but now that we have draws from $p_{low}$, we are
done with it until the importance sampling stage.

With a simple one parameter model we can plot our approximate errors as a
function of K (so we know the solution is suitable everywhere).

```{r}
dt_ref = 0.005
Nx_ref = 80
K_draws = rstan::extract(fit, "K")$K
sigma_draws = rstan::extract(fit, "sigma")$sigma
num_draws = length(K_draws)

max_abs_error = 0.0
errors = c()
for(i in 1:num_draws) {
  K = K_draws[i]
  mu_low_res = solve_pde(dt_ref, Nx_ref, K, T_meas, x_meas)
  mu_high_res = solve_pde(dt_ref / 2.0, Nx_ref * 2, K, T_meas, x_meas)
  errors = c(errors, max(abs(mu_high_res - mu_low_res)))
}

plot(K_draws, errors)
```

The maximum error here seems good enough.

## 3. Compute importance weights $\frac{p_{high}}{p_{low}}$ and $\hat{k}$ diagnostic.

The importance weights are computed on the log scale:

```{r}
log_lh = rep(0, num_draws)
log_lh_ref = rep(0, num_draws)
for (i in seq_len(num_draws)) {
  mu = solve_pde(dt, Nx, K_draws[i], T_meas, x_meas)
  mu_ref = solve_pde(dt_ref, Nx_ref, K_draws[i], T_meas, x_meas)

  log_lh[i] = sum(dnorm(y, mu, sigma_draws[i], log = TRUE))
  log_lh_ref[i] = sum(dnorm(y, mu_ref, sigma_draws[i], log = TRUE))
}
```

The `loo` package computes the $\hat{k}$ diagnostic for us:

```{r pareto, fig.width=6, fig.height=4.5}
loo::psis(log_lh_ref - log_lh)
```

Oh no! $\hat{k} > 0.5$, and it turns out modeling this process with one timestep
and one spatial point is not a good idea.

## 4. Break if $\hat{k} < 0.5$, otherwise raise precision of $p_{low}$ and loop

It is time to up the precision in the low resolution model:

```{r}
dt = 0.1
Nx = 10

fit <- sampling(model,
            data = list(dt = dt,
                        Nx = Nx,
                        N_meas = length(x_meas),
                        T_meas = T_meas,
                        x_meas = x_meas,
                        y = y),
            cores = 4)
```

Again we can check our regular diagnostics:

```{r}
print(fit, pars = c("K", "sigma"))
```

And again we can verify our reference solution:

```{r}
dt_ref = 0.005
Nx_ref = 80
K_draws = rstan::extract(fit, "K")$K
sigma_draws = rstan::extract(fit, "sigma")$sigma
num_draws = length(K_draws)

max_abs_error = 0.0
errors = c()
for(i in 1:num_draws) {
  K = K_draws[i]
  mu_low_res = solve_pde(dt_ref, Nx_ref, K, T_meas, x_meas)
  mu_high_res = solve_pde(dt_ref / 2.0, Nx_ref * 2, K, T_meas, x_meas)
  errors = c(errors, max(abs(mu_high_res - mu_low_res)))
}

plot(K_draws, errors)
```

And again we can compute the importance ratios and run the PSIS diagnostics on
them:

```{r}
log_lh = rep(0, num_draws)
log_lh_ref = rep(0, num_draws)
for (i in seq_len(num_draws)) {
  mu = solve_pde(dt, Nx, K_draws[i], T_meas, x_meas)
  mu_ref = solve_pde(dt_ref, Nx_ref, K_draws[i], T_meas, x_meas)

  log_lh[i] = sum(dnorm(y, mu, sigma_draws[i], log = TRUE))
  log_lh_ref[i] = sum(dnorm(y, mu_ref, sigma_draws[i], log = TRUE))
}

loo::psis(log_lh_ref - log_lh)
```

And this time $\hat{k} < 0.5$, so we are good enough!

## 5. Resample draws using importance weights

The last step is resampling. We have draws of $K$ and $\sigma$, but they are
weighted, and usually it is easier to just use a set of draws. Just because it
is possible to do an importance sampling correction on a set of draws does not
mean that unweighted statistics on these draws are safe to use. In this case,
the results are not much different, but it is relatively trivial to do a
resampling with the `posterior` package:

```{r}
draws_df = posterior::as_draws_df(rstan::extract(fit, c("K", "sigma")))
resampled_df = posterior::resample_draws(draws_df,
                                         weights = exp(log_lh_ref - log_lh))

print(draws_df %>% posterior::summarize_draws())
print(resampled_df %>% posterior::summarize_draws())
```
```

# Computation environment

```{r session}
sessionInfo()
```

# References
