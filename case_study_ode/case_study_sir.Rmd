---
title: "Reliable and efficient ODE model inference"
date: "`r Sys.Date()`"
author: "Juho Timonen, Ben Bales, Harri Lähdesmäki and Aki Vehtari"
link-citations: true
output:
  html_document:
    number_sections: true
    toc: true
    toc_float: true
    toc_depth: 4
    highlight: pygments
    theme: cosmo
    css: "style.css"
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load, message=FALSE}
# Requirements
library(cmdstanr)
library(posterior)
library(bayesplot)
library(scales)
library(ggplot2)
library(loo)
library(stats)
library(outbreaks)

# Options
stan_opts <- list(
  seed = 123,
  sig_figs = 18
)
```

# Introduction

## ODEs

Ordinary differential equations (ODEs) 

\begin{equation}
\frac{\text{d} \textbf{x}(t)}{\text{d} t} = f_{\nu}(\textbf{x}(t), t)
\end{equation}

are a model for the rate of change in some quantities $\textbf{x}(t)$.
Many fields of science utilize ODEs and have laws that define the function $f_{\nu}$ for a specific problem, but it is common that its parameters $\nu$
are unknown and need to be inferred. It is common that we have time-course data
$\{(t_n, \textbf{y}_n)\}_{n=1}^N$, observed at time points $t_n \neq t_0$, and
we need to calculate $\textbf{x}(t_n)$ for each $n=1, \ldots, N$, in order to
compute likelihood of the data. Given initial value  
$\textbf{x}(t_0) = \textbf{x}_0$, and assuming some smoothness requirements for
$f_{\nu}$, there exists a unique solution for $\textbf{x}(t)$, 
$t \in \mathbb{R}$. This solution is given by the integral formula
\begin{equation}
\textbf{x}(t) = \textbf{x}_0 + \int_{t_0}^t f_{\nu}(\textbf{x}(\tau), \tau) 
\text{d} \tau,
\end{equation}

but the integral usually doesn't have a closed form, and numerical methods are required.

## SIR example

We use data from the **outbreaks** package. In the variable `I_data`,
we store the number of infected people on 14 consecutive days.
```{r sir_data}
I_data <- influenza_england_1978_school$in_bed
N <- length(I_data)
data_list <- list(
  I_data = I_data,
  N = N,
  t = seq(1, N),
  I0 = 1,
  pop_size = 763
)
print(data_list)
```

We create a classic Susceptible-Infected-Recovered (SIR) model of disease
spread. The code is adapted from that in the Stan Case Study [@grinsztajn2020], 
which provides an introduction to disease
transmission modeling in Stan in general. The states of the ODE are expected
amounts of susceptible (S), infected (I) and recovered
(R) people. The dynamics are given by the ODE system:

\begin{align}
\frac{dS}{dt} &= -\beta \cdot I \cdot \frac{S}{N_{pop}} \\
\frac{dI}{dt} &=  \beta  \cdot I \cdot \frac{S}{N_{pop}} - \gamma \cdot I \\
\frac{dR}{dt} &=  \gamma \cdot I,
\end{align}
where $N_{pop}$ is the population size. Using the above notation, we have
$\textbf{x}(t) = [S(t), I(t), R(t)]^{\top}$ and $\nu = \{\beta, \gamma\}$. 

The actual number of infected people, which we denote by $y(t)$
is then assumed to follow a negative binomial distribution with mean
$I(t)$ and inverse overdispersion $\phi$. All model parameters are collected
in a vector $\theta = \{\beta, \gamma, \phi\}$.

## Stan code

```{r sir_stan_code, message=FALSE}
sir_pars <- "
  real<lower=0> beta;
  real<lower=0> gamma;
  real<lower=0> phi_inv;
"
sir_tpars <- "  real phi = inv(phi_inv);"
sir_prior <- "
  beta ~ normal(2, 1);
  gamma ~ normal(0.4, 0.5);
  phi_inv ~ exponential(5);
"
sir_funs <- "
  // SIR system right-hand side
  vector SIR(real t, vector y, data int[] a0, vector theta) {
    vector[3] dx_dt;
    int pop_size = a0[1];
    real infection_rate = theta[1] * y[2] * y[1] / pop_size;
    real recovery_rate = theta[2] * y[2];
    dx_dt[1] = - infection_rate;
    dx_dt[2] = infection_rate - recovery_rate;
    dx_dt[3] = recovery_rate;
    return dx_dt;
  }
"
sir_data <- "
  int<lower=1> N; // number of time points
  int<lower=3,upper=3> D; // dimension of ODE
  real t[N]; // time points
  int<lower=1> pop_size; // population size
  real<lower=1> I0; // initial number of infected
  real<lower=0> RTOL; // ODE solver relative tolerance
  real<lower=0> ATOL; // ODE solver absolute tolerance
  int<lower=1> MAX_NUM_STEPS; // ODE solver maximum number of steps
"
sir_tdata <- "
  real t0 = 0.0;
  int a0[1] = {pop_size};
  vector[D] x0 = to_vector({pop_size - I0, I0, 0.0}); // {S, I, R}
"
sir_obsdata <- "  int<lower=0> I_data[N];"
sir_odesolve <- "  vector[D] x[N] = ode_rk45_tol(SIR, x0, t0, t, RTOL, ATOL, 
    MAX_NUM_STEPS, a0, to_vector({beta, gamma}));"
sir_likelihood <- "
  for(n in 1:N) {
    I_data[n] ~ neg_binomial_2(x[n][2] + 10*ATOL, phi);
  }
"
sir_gq <- "
  int y[D, N];
  real log_lik = 0.0;
  for(n in 1:N) {
    for(d in 1:D) {
      y[d,n] = neg_binomial_2_rng(x[n][d] + 10*ATOL, phi); 
    }
    log_lik += neg_binomial_2_lpmf(I_data[n] | x[n][2] + 10*ATOL, phi);
  }
"
data_list$D <- 3
```

We create and compile three Stan models.
```{r sir_compile, message=FALSE}
source("R/odeadapt.R")
sir_models <- create_cmdstan_models(sir_funs, sir_data, sir_tdata, sir_obsdata,
                                    sir_pars, sir_tpars,
                                    sir_prior, sir_odesolve, 
                                    sir_likelihood, sir_gq)
```

## Solving the ODE system
We wish to demonstrate solving the ODE system, but because we don't know the
values of the parameters, we first draw them from a prior distribution
using the first Stan model.

```{r print_stan_model_prior}
sir_models$prior
```

We sample the model and extract prior draws of the parameters.

```{r sample_prior}
prior_fit <- sir_models$prior$sample(
  sig_figs = stan_opts$sig_figs, 
  seed = stan_opts$seed,
  refresh = 1000
)
print(prior_fit)
prior_draws <- prior_fit$draws(c("beta", "gamma", "phi_inv"))
```

These draws will be passed to the second Stan model
```{r print_stan_model_sim}
sir_models$sim
```
that can be called in the `generate_quantities` mode to 
solve the system using the prior parameter draws. We won't yet go into detail
about the meaning of the 
`RTOL`, `ATOL` and `MAX_NUM_STEPS` control arguments but just give them
some values and plot the obtained ODE solutions.

```{r sir_sim_model, message=FALSE}
# Simulate solutions using prior draws
prior_sim <- simulate(sir_models$sim, prior_draws, data_list, stan_opts)

# Function that plots solutions against data
plot_sir_example_solutions <- function(sim, data, thin=1, main="") {
  x_sim <- thin_draws(merge_chains(sim$draws("x")), thin)
  I_sim <- x_sim[,1,(N+1):(2*N), drop=TRUE]
  plot(data$t, rep(data$pop_size, data$N), type="l", lty=2,
       col="gray70", ylim=c(0,800), ylab="Infected", xlab="Day",
       main = main)
  for(i_draw in 1:nrow(I_sim)) {
    I <- as.vector(I_sim[i_draw, ])
    lines(data$t, I, col=alpha("firebrick", 0.1))
  }
  points(data$t, data$I_data, ylim = c(0, 1000), pch=20)
}

# Call the above function
title <- "Solutions using prior param draws"
plot_sir_example_solutions(prior_sim, data_list, thin=10, main=title)
```

## Analyzing the numerical method
The solution provided by a numerical method is always an approximation to the
true solution. For the ODE solvers in Stan, the control arguments `RTOL` 
and `ATOL` have an effect on the accuracy of this solution. In addition,
`MAX_NUM_STEPS` affects whether a solution can be obtained at all. Here
we use `MAX_NUM_STEPS=1e6` and should not have problems with it being
reached.

We repeat the prior ODE solving with different values of `RTOL` and `ATOL`.

```{r sir_test_prior_tols_time, results = FALSE}
TOLS <- 10^seq(-8,-4) # 10^seq(-14,-3)
atols <- TOLS
rtols <- TOLS
sims <- simulate_many(sir_models$sim, prior_draws, data_list, stan_opts,
                      atols, rtols)
```

We plot the simulation runtime for each case.

```{r print_res, fig.width=8.2, fig.height=4.5}
plot_sim_times(atols, rtols, sims$times)
```

We see that reducing either tolerance parameter generally improves runtime,
until some point after which it doesn't have much effect anymore. We also
plot here the mean and maximum absolute error with respect to the solution
given by the strictest tolerances.

```{r sir_sol_errors, fig.width=8.2, fig.height=4.5}
mean_abs_sol_error <- compute_sol_errors(sims$sims, "mean")
max_abs_sol_error <- compute_sol_errors(sims$sims, "max")
plot_sim_errors(atols, rtols, mean_abs_sol_error)
plot_sim_errors(atols, rtols, max_abs_sol_error)
```

Finally we also plot the error in log likelihood
```{r sir_loglik_errors, fig.width=8.2, fig.height=4.5}
mean_abs_loglik_error <- compute_loglik_errors(sims$log_liks, "mean")
max_abs_loglik_error <- compute_loglik_errors(sims$log_liks, "max")
plot_sim_errors(atols, rtols, mean_abs_loglik_error, log=FALSE)
plot_sim_errors(atols, rtols, max_abs_loglik_error, log=FALSE)
```

# Computational challenges

When performing Bayesian inference for the parameters $\theta$
(and possible other model parameters, note that also $\textbf{y}_0$ can be a
parameter) using MCMC methods, the system needs to be solved numerically on each log posterior probability evaluation. Furthermore, when using a gradient
based MCMCM algorithm like Stan does, also _sensitivities_, i.e. 
derivatives of the solution w.r.t. parameters, are needed. This is done
in Stan by numerically solving an extended ODE system consisting of the
sensitivities and the original dynamical variables. 

The solution provided by a numerical method
is always an approximation to the true solution. 
This means that the computed posterior probability density and its
gradient are also approximations and the whole MCMC inference is biased
to some degree. So we are performing Bayesian inference for the parameters, the error in the solutions propagates also to the statistical properties of the posterior draws, in ways that cannot be predicted beforehand.

How could this error be studied? If the model parameters are fixed,
we can verify that the solution at all points is
appropriately close to a more accurate reference solution. 
There is a vast amount of classical literature that studies this error, and the ODE solvers built into
Stan are these classical methods that obtain estimate their error and adapt
their step size so that a given tolerance is achieved. However, we are doing statistics, and so we need to
know that the solution is accurate enough across all relevant parts of
parameter space. Additionally, it is not known beforehand where the 
"relevant parts of parameter space" are!

The problem of validating the use of a numerical method for a Bayesian model
is therefore significantly more complicated than in the classical numerical
analysis world. One idea is to run sampling repeatedly, gradually increasing
the solver accuracy, until the posterior inference
results don't change anymore. But this might not
be computationally very attractive. The numerical methods are often expensive
and frequently become the limiting factors in whatever model they are involved in. Moreover, achieving more accuracy often comes with the cost of more
computation time. Note however that there is no direct tradeoff.

These two problems (computational cost and potential unreliability) occur also
with any other type of model that requires numerically solving an implicitly defined function or variable, but in this case study we focus on ODEs.

The point of this case study is to show how by adding one
additional tool, namely Pareto-Smoothed Importance Sampling (PSIS) [@yao2018;
@vehtari2019], we can solve this problem.


## Simpler numerical methods
Simplest numerical methods for integrating ODEs usually involve some
discretization in time, which affects the solver accuracy. Denser 
discretization means more accuracy but also more computation, as more time
steps need to be taken.

The simplest things we might do to speed up our calculations are lowering
the timestep, coarsening the discretization, or increasing the tolerance of the
solvers. That immediately leaves us with the question, is this okay? Has
changing the numerical method affected our parameter inference results? Was our
original method giving correct inference results to begin with? Are the default
tolerances in Stan suitable for the problem at hand?

# Workflow

Let $M$ be the model for which we would like to perform inference, but which we
cannot evaluate since the likelihood is defined implicitly through an
ODE or PDE system that is not analytically tractable. MCMC inference for $M$
can be seen actually as inference for another model $M_{high}$, which
is the same model as $M$ but using a numerical solver, and can therefore be
evaluated.

Our workflow addresses the problem of defining the high-precision numerical
method in $M_{high}$ so that $M_{high}$ can trusted to have essentially the
same posterior as $M$. We define a way to perform inference for $M_{high}$ 
without needing to compute gradients or HMC trajectories for it. This involves
another model $M_{low}$, which is again the same model, except that $M_{low}$
uses a cheaper and less accurate numerical methods (or just looser tolerances
and/or coarser discretization) to compute the required ODE or PDE solutions,
and is therefore faster to fit. The posterior densities are denoted $p_{low}$
and $p_{high}$, respectively.

To understand how PSIS comes into play, we must first discuss importance
sampling. If we want to compute expectations with the high precision model, we
can take draws from the low precision models and reweight these according to the
importance weights $\frac{p_{high}}{p_{low}}$. If these models are too
different, then the reweighting will produce noisy estimates that are not
useful. PSIS and particularly the Pareto $k$-diagnostic (denoted $\hat{k}$), is
the tool that tells us when we can or cannot rely on the importance weights. If
$\hat{k} < 0.5$ we are safe to do the importance sampling, if $\hat{k} < 0.7$
the importance sampling will start to converge more slowly, and if
$\hat{k} > 0.7$ the importance sampling estimates are unreliable. For simplicity
we will only consider the $\hat{k} < 0.5$ threshold.

Ideally, $M_{high}$ would involve a numerical method that we can trust
completely in all parts of the parameter space so that, as long as
$\hat{k} < 0.5$, importance weights can be used to reweight the low precision
approximation $p_{low}$ to the high precision approximation $p_{high}$. We can
think of $M_{high}$ as a reference model, because it is the baseline to which
we compare. It is difficult in practice to know if a given model is a good
reference model in all parts of parameter space, due to curse of dimensionality
and the fact that analysed system can have different properties in different
parts of the parameter space. For example, ODEs can qualitatively change their
behaviour as a function of parameters (bifurcation), or become stiff or
chaotic in some parameter regions. Accuracy can be checked at a given set of
parameters fairly easily, but not over a high dimensional parameter space.
Under these conditions it is necessary to compromise to develop a reference
model that works only over a range of parameter space, but even then it is hard
to know *a priori* what range that is.

We propose the following workflow:

::: {#workflow .box}
1. Generate draws from $p_{low}$.
2. Tune the numerical method in $M_{high}$ so that it is reliable at
these draws. All application specific knowledge and classical numerical
analysis can be used here.
3. Compute importance weights $\frac{p_{high}}{p_{low}}$
4. Compute the $\hat{k}$ diagnostic. If $\hat{k} > 0.5$, raise precision of
the numerical method in $M_{low}$ and go back to step 1.
5. Resample draws of $M_{low}$, using importance weights to get draws from
$M_{high}$, and therefore essentially draws from $M$.
:::

The next two sections of this case study outline how to apply this workflow to
do fast but reliable inference for

1. an ODE model using a built-in Stan solver
2. a PDE model using a custom solver that does not have explicit tolerance
controls

The importance sampling diagnostics are handled with the
[`loo`](https://mc-stan.org/loo) package and the resampling is handled with the
[`posterior`](https://github.com/stan-dev/posterior) package.


## Example: SIR fitting
The parameters $\beta$ and $\gamma$
will be estimated from time series observations of the number of infected
people (I). For the purposes of this case study, the goal is to use a very low
precision ODE solver to do inference and then check it afterwards against
a high precision solver. This is useful in practice if sampling with the
high precision solver itself would take an inordinate amount of time.

```{r print_posterior_model}
sir_models$posterior
```

We sample the model and extract posterior draws of the parameters.

### 1. Generate draws from $p_{low}$.

```{r sample_posterior, message=FALSE}
solver_args_sample <- list(
  ATOL = 1e-6,
  RTOL = 1e-6,
  MAX_NUM_STEPS=1e6
)
posterior_fit <- sir_models$posterior$sample(
  data = c(data_list, solver_args_sample),
  sig_figs = stan_opts$sig_figs, 
  seed = stan_opts$seed,
  refresh = 1000
)
```

```{r print_post}
print(posterior_fit)
posterior_draws <- posterior_fit$draws(c("beta", "gamma", "phi_inv"))
```

These draws will be passed again to `sir_models$sim`

```{r sir_sim_posterior, message=FALSE}
# Simulate and plot generated solutions
posterior_sim <- simulate(sir_models$sim, posterior_draws, data_list, stan_opts,
                          solver_args = solver_args_sample)

title <- "Solutions using posterior param draws"
plot_sir_example_solutions(posterior_sim, data_list, thin=10, main=title)
```

```{r sir_sim_posterior_check, message=FALSE}
solver_args_check <- list(
  ATOL = 1e-10,
  RTOL = 1e-10,
  MAX_NUM_STEPS=1e12
)
posterior_sim_check <- simulate(sir_models$sim, posterior_draws, data_list,
                                stan_opts, solver_args = solver_args_check)
```

2. Tune the numerical method in $M_{high}$ so that it is reliable at
these draws. All application specific knowledge and classical numerical
analysis can be used here.
3. Compute importance weights $\frac{p_{high}}{p_{low}}$
4. Compute the $\hat{k}$ diagnostic. If $\hat{k} > 0.5$, raise precision of
the numerical method in $M_{low}$ and go back to step 1.
5. Resample draws of $M_{low}$, using importance weights to get draws from
$M_{high}$, and therefore essentially draws from $M$.

# Warning messages

During posterior sampling, it is typical that the following 
```{messages_header}
## Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:
```
occurs multiple times, followed by one of these
```{messages_content}
## Chain 1 Exception: ode_rk45_tol: ode parameters and data[1] is inf, but must be finite! (in '/var/folders/cj/c_w12skd6mj6x942dt2twtp40000gn/T/RtmpPBWP3g/model-13ef24959208c.stan', line 50, column 2 to line 51, column 49)

## Chain 1 Exception: neg_binomial_2_lpmf: Location parameter is nan, but must be positive finite! (in '/var/folders/cj/c_w12skd6mj6x942dt2twtp40000gn/T/RtmpPBWP3g/model-13ef24959208c.stan', line 54, column 4 to column 55)

## Chain 2 Exception: ode_rk45_tol:  Failed to integrate to next output time (1) in less than max_num_steps steps (in '/var/folders/cj/c_w12skd6mj6x942dt2twtp40000gn/T/RtmpPBWP3g/model-13ef24959208c.stan', line 50, column 2 to line 51, column 49)
```
messages.

## References
