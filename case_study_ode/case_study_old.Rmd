---
title: "Reliable and efficient ODE model inference"
date: "`r Sys.Date()`"
author: "Juho Timonen, Ben Bales, Harri Lähdesmäki and Aki Vehtari"
link-citations: true
output:
  html_document:
    number_sections: true
    toc: true
    toc_float: true
    toc_depth: 4
    highlight: pygments
    theme: cosmo
    css: "style.css"
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Solving the system

We will import a function from `sir.stan` for solving the ODE.
It uses the `integrate_ode_rk45` function that is built into Stan.
The actual function exposed from the Stan model (`stan_solve_sir`) is a bit
awkward so we rewrap it here in a way that is easier to use.

```{r sir_solve}
expose_stan_functions(model)

# Solve the SIR system
# - theta = c(beta, gamma), parameters
# - opts = c(rtol, atol, max_num_steps), solver options
# - ts = vector of output time points
solve_sir <- function(ts, theta, opts) {
  stan_solve_sir(ts, theta, c(0), opts[1], opts[2], opts[3])
}
```

The RK45 solver in Stan is an adaptive time step solver, which estimates the
local error of the solution and adapts its step size so that the local error
estimates are less than `atol + rtol * abs(y)`, where `y` is the ODE solution,
and `atol` and `rtol` are called absolute and relative tolerance,
respectively. These tolerances need to be given, and affect both the accuracy
and computational cost of the solution. In general, `rtol` is the tolerance on
the relative error the solver can make when `y` is far from zero. When `abs(y)`
is small (of the order of `atol` or smaller), there is no need to achieve the
relative tolerance.

A third control parameter, `max_num_steps`, determines the maximum number of
steps that can be taken to achieve the tolerance. In practice, we have
observed that setting this to a much smaller value than the default can
lower the warmup times of some chains by several orders of magnitude. This can
be because it possibly helps in rejecting or quickly getting out of 
initial low-probability parameter regions, where the ODE solutions blow up and achieving the tolerances would require a much larger number of steps than in
the good parameter region.

We can just pick some options and quickly run and plot a solution
just to get a feel for what the system looks like:

```{r sir_plot}
plot_sir <- function(t, y) {
  y %>%
    as_tibble() %>%
    setNames(c("I")) %>%
    mutate(Day = t) %>%
    ggplot(aes(Day, I)) +
    geom_line() +
    geom_point() +
    ylab("Infected people")
}

ts <- seq(0.1, 16, by = 0.1)
theta_true <- c(1, 0.2) # true parameter values
opts <- c(1e-4, 1e-4, 100)
ys <- solve_sir(ts, theta_true, opts)
plot_sir(ts, ys)
```

## Generating test data

To test fitting our model we will create noisy measurements of the number of infected people (I) at each day. If we're going to generate data from our model we better have an accurate ODE solver, otherwise we're just generating data
from some weird approximate model.

The simplest way to check that an `atol` and `rtol` are suitable is to do
a solve at one tolerance level, repeat the solve at a much smaller (more
precise) tolerance, and then look at the maximum absolute error at any output
point. We will create a function to do this automatically:

```{r sir_check}
check_reliability_sir <- function(theta, opts) {
  ts <- seq(0.1, 16, by = 0.1)
  y_hat <- solve_sir(ts, theta, opts)
  opts_strict <- c(opts[1] / 10, opts[2] / 10, opts[3])
  y_hat_strict <- solve_sir(ts, theta, opts_strict)
  max_abs_err <- max(abs(y_hat - y_hat_strict))
  return(max_abs_err)
}
```

We can study the maximum absolute error compared to a solution with 10 times
smaller tolerances, as a function `tol = atol = rtol`. Value of `max_num_steps` is kept constant, but if a solver should fail to compute a solution in
those steps, an error is thrown and it needs to be increased. 

```{r sir_tols, fig.width=7, fig.height=3.5}
mae_true <- c()
tols <- 10^(-c(1:12))
for (tol in tols) {
  opts <- c(tol, tol, 1e7)
  mae_true <- c(mae_true, check_reliability_sir(theta_true, opts))
}

qplot(tols, mae_true, geom = c("point", "line")) +
  scale_x_log10() +
  scale_y_log10() + ylab("Max. absolute error") + xlab("Tolerance")
```

From this and our prior knowledge of infectious diseases, we assert
that $10^{-6}$ is a good enough value to use for `atol` and `rtol` during
simulation. Certainly we do not expect have a count of the infected
population accurate to $10^{-4}$ people.

We generate the observed number of infected people (cases) at each time point
$t_i$, from a negative binomial distribution with mean equal to the solution of
$S(t_i)$ from the ODE, and dispersion parameter $\phi = 5$.

```{r sir_noise, fig.width=7, fig.height=4.8}
atol <- 1e-6
rtol <- 1e-6
opts <- c(atol, rtol, 1e7)

N <- 16 # number of data points
t_data <- seq(1, N)
dispersion <- 5 # noise parameter for negative binomial
mu <- solve_sir(t_data, theta_true, opts)
y_data <- rnbinom(length(t_data), mu = mu, size = dispersion)

tibble(t = t_data, mu = mu, y = y_data) %>%
  ggplot() +
  geom_line(aes(t, mu), col = "firebrick") +
  geom_point(aes(t, y)) +
  xlab("Day") +
  ylab("Infected people") +
  ggtitle("Simulated data as points \nUnderlying solution as lines")
```

## Likelihood

We also define an R function that computes the likelihood given the data,
parameter values and solver options.
```{r sir_likelihood, fig.width=7, fig.height=4.5, results=FALSE}
# Likelihood function for the SIR model
# - t_data = vector of measurement times
# - y_data = vector of measurements of number of infected people
# - params = parameter vector c(beta, gamma, phi)
# - opts = c(rtol, atol, max_num_steps), solver options
log_likelihood_sir <- function(t_data, y_data, params, opts) {
  theta <- params[1:2]
  phi <- params[3]
  y_hat <- solve_sir(t_data, theta, opts)
  log_lh <- sum(dnbinom(y_data, size = phi, mu = y_hat, log = TRUE))
  return(log_lh)
}
```

## Applying the workflow

As a reminder, our mission in fitting this ODE is to use a low precision
solver. It is always tempting to use low precision solvers when working
with ODEs because they (usually) run faster. The difficulty becomes how to
deal with the coarser approximation. Does the lower precision cause an effect
that matters? If so, can it be corrected and how? These are the questions the
workflow here will allow us to answer.

### Generating draws from $p_{low}$

The first step in the workflow is to take any low precision approximation 
($M_{low}$) and fit the data. Remember, all our numerical methods are
approximations, and so we refer to this model specifically as a low precision
model. We will check it against a higher precision model later. In this case,
we will use `rtol = 1e-4`, `atol = 1e-3`.

```{r sir_fit, fig.width=7, fig.height=4.5, results=FALSE}
opts_low <- c(1e-4, 1e-3, 100)
stan_data <- list(
  N = length(t_data),
  t_data = t_data,
  y_data = y_data,
  rtol = opts_low[1],
  atol = opts_low[2],
  max_num_steps = opts_low[3]
)

fit1 <- rstan::sampling(model,
                       stan_data,
                       seed = rng_seed_stan,
                       cores = 4
)
```


```{r sir_summary}
pars <- c("beta", "gamma", "phi")
draws1 <- get_draws(fit1, pars)
print(fit1, pars = c("beta", "gamma", "phi"))
```

### Developing a reference model

Before we can check if the importance sampling correction is possible, we
need to have a reference model ($M_{high}$) to compare against. That means we
need a version of the model with tolerances such that it is suitably accurate
across all the posterior draws generated from the low precision model.

In this case, `rtol = atol = 1e-6` was accurate enough generating the data,
so let's check if it is accurate in all the draws in this posterior.

```{r sir_tune}
# Check reliability at each draw and add MAEs to data frame
compute_errors_sir <- function(df, opts) {
  num_draws <- nrow(df)
  mae <- c()
  for (i in 1:num_draws) {
    theta <- as.numeric(df[i, c("beta", "gamma")])
    mae_i <- check_reliability_sir(theta, opts)
    mae <- c(mae, mae_i)
  }
  df$mae <- mae
  return(df)
}

opts_high <- c(1e-6, 1e-6, 1e8)
ode_pars <- c("beta", "gamma")
draws1 <- compute_errors_sir(draws1, opts_high)

p1 <- scatter_colored(draws1, "beta", "gamma", "mae")
p2 <- scatter_colored(draws1, "beta", "phi", "mae")
p3 <- scatter_colored(draws1, "gamma", "phi", "mae")
ggarrange(p1, p2, p3)
```

We can plot this as a distribution and see that `rtol = atol = 1e-6` keeps
us under an absolute error of one milliperson. This seems accurate enough.

```{r sir_hist, message=FALSE}
qplot(draws1$mae, geom = "histogram")
```

### Computing importance weights

With the reference model in place, it is time to compute the importance
weights $\frac{p_{high}}{p_{low}}$ for each post-warmup draw. This is simple:
just compute the log density of the reference model and the log
density of the low precision model and take the difference (we work
with the log of the importance ratios $\log p_{high} - \log p_{low}$
for numeric stability).

The hidden downside is that it might take some time to compute the log
densities of the reference model for each draw. It should still be way faster
than sampling with the reference model itself, since we don't need to compute
gradients, HMC trajectories, evaluate proposals that can be rejected, and 
skip the whole warmup period. Therefore we likely won't have to try to do
accurate ODE solves in the ill-behaved parameter regions, where a huge number
of steps would be needed to achieve the tolerances. Another upside is that
the calculations could be done in parallel for each draw.

If the priors are kept the same between the reference and low precision
model, then those can be left out of this calculation (they will cancel).

```{r sir_weights}
# Compute log likelihood ratio for each draw and add to data frame
log_ratios_sir <- function(df) {
  num_draws <- nrow(df)
  log_lh_low <- rep(0, num_draws)
  log_lh_high <- rep(0, num_draws)
  for (i in seq_len(num_draws)) {
    params_i <- as.numeric(df[i, c("beta", "gamma", "phi")])
    log_lh_low[i] <- log_likelihood_sir(t_data, y_data, params_i, opts_low)
    log_lh_high[i] <- log_likelihood_sir(t_data, y_data, params_i, opts_high)
  }
  df$log_ratio <- log_lh_high - log_lh_low
  return(df)
}

draws1 <- log_ratios_sir(draws1)
p1 <- scatter_colored(draws1, "beta", "gamma", "log_ratio")
p2 <- scatter_colored(draws1, "beta", "phi", "log_ratio")
p3 <- scatter_colored(draws1, "gamma", "phi", "log_ratio")
ggarrange(p1, p2, p3)
```

We can plot the log importance ratios and see they are all close to zero
(which means out approximation was not too bad).

```{r sir_weight_hist, message=FALSE}
qplot(draws1$log_ratios, geom = "histogram")
```

### Computing $\hat{k}$ diagnostic

With the importance ratios calculated, we can check if they are usable or not
with the PSIS $\hat{k}$ diagnostic.

```{r sir_pareto}
r_eff1 <- loo::relative_eff(x = exp(-draws1$log_ratio), draws1$chain_id)
psis1 <- loo::psis(draws1$log_ratio, r_eff = r_eff1)
print(psis1$diagnostics)
print(psis1)
```

$\hat{k} < 0.5$, and so importance sampling should be reliable.

### Resampling

At this point we have a weighted set of posterior draws. It
is usually easier to work with a set of draws than a set of weighted draws, so
we can resample our weighted draws to become a set of unweighted draws using
`posterior::resample_draws`. The effective sample size will be slightly
lowered by such a resampling, but unweighted draws are really more convenient
to work with.

Just because it is possible to do an importance sampling correction on a set
of draws does not mean that unweighted statistics on these draws are safe
to use. In this case, the results are not much different, but it should not
be forgotten:

```{r sir_resample}
w1 <- exp(draws1$log_ratio)
draws1_list <- rstan::extract(fit1, c("beta", "gamma", "phi"))
draws_df1 <- posterior::as_draws_df(draws1_list)
resampled_df1 <- posterior::resample_draws(draws_df1, weights = w1)

print(draws_df1 %>% posterior::summarize_draws())
print(resampled_df1 %>% posterior::summarize_draws())
```

# Conclusions

And that is that! Happy approximating!

# Computation environment

```{r computational_environment, echo=FALSE}
sessionInfo()
cat(paste0("Initial rng seed was ", rng_seed_r, "\n"))
cat(paste0("Stan rng seed was ", rng_seed_stan, "\n"))

# Result check
pareto_k_check <- function(res, expected_ok = TRUE) {
  arg_name <- deparse(substitute(res))
  khat <- res$diagnostics$pareto_k
  msg <- paste0("In ", arg_name, ", pareto_k was ", round(khat, 5), "\n")
  cat(msg)
  return(khat)
}
khat1 <- pareto_k_check(psis1)
if (khat1 > 0.5) {
  stop("Results inconsistent with text!")
}
```

# References
